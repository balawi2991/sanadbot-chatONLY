import { NextRequest, NextResponse } from "next/server"
import { prisma } from "@/lib/prisma"
import { GoogleGenerativeAI } from "@google/generative-ai"

// Initialize Gemini AI
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || "AIzaSyCwhDop1HFDiGBGUPClIW6zIxouOI_Ohdg")

// Function to find similar Q&A
function findSimilarQA(question: string, qas: any[]) {
  const normalizedQuestion = question.toLowerCase().trim()
  
  // First try exact match
  for (const qa of qas) {
    if (qa.question.toLowerCase().trim() === normalizedQuestion) {
      return qa
    }
  }
  
  // Then try partial match
  for (const qa of qas) {
    const qaQuestion = qa.question.toLowerCase()
    if (qaQuestion.includes(normalizedQuestion) || normalizedQuestion.includes(qaQuestion)) {
      return qa
    }
  }
  
  // Finally try keyword matching
  const questionWords = normalizedQuestion.split(' ').filter(word => word.length > 2)
  for (const qa of qas) {
    const qaWords = qa.question.toLowerCase().split(' ')
    const matchCount = questionWords.filter(word => qaWords.some(qaWord => qaWord.includes(word))).length
    if (matchCount >= Math.min(2, questionWords.length)) {
      return qa
    }
  }
  
  return null
}

// Function to generate RAG response with performance monitoring
async function generateRAGResponse(question: string, bot: any, knowledgeSources: any[], conversationHistory: any[] = []) {
  const performanceStart = performance.now()
  console.log('ğŸš€ [Gemini] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨:', new Date().toISOString())
  
  try {
    const setupStart = performance.now()
    const model = genAI.getGenerativeModel({ 
      model: "gemini-1.5-flash",
      generationConfig: {
        temperature: 0.7,
        topP: 0.8,
        maxOutputTokens: 1024,
      }
    })
    const setupEnd = performance.now()
    console.log('âš™ï¸ [Gemini] Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:', (setupEnd - setupStart).toFixed(2), 'ms')
    
    // Prepare context from knowledge sources
    const contextStart = performance.now()
    let context = ""
    knowledgeSources.forEach((source, index) => {
      context += `Ù…ØµØ¯Ø± ${index + 1} (${source.title}):\n${source.content}\n\n`
    })
    
    // Prepare conversation history for context
    let historyContext = ""
    if (conversationHistory.length > 0) {
      historyContext = "\n\nØ³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n"
      conversationHistory.slice(-3).forEach((conv, index) => {
        historyContext += `Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: ${conv.question}\nØ§Ù„Ù…Ø³Ø§Ø¹Ø¯: ${conv.answer}\n\n`
      })
    }
    
    const contextEnd = performance.now()
    console.log('ğŸ“ [Gemini] Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚:', (contextEnd - contextStart).toFixed(2), 'ms')
    console.log('ğŸ“Š [Gemini] Ø­Ø¬Ù… Ø§Ù„Ø³ÙŠØ§Ù‚:', context.length + historyContext.length, 'Ø­Ø±Ù')
    console.log('ğŸ’¬ [Gemini] Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:', conversationHistory.length)
    
    // Prepare the prompt
    const prompt = `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ø§Ø³Ù…Ùƒ "${bot.name}". Ø´Ø®ØµÙŠØªÙƒ: ${bot.personality}

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ø¯ÙŠÙƒ:
${context}${historyContext}

Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ: ${question}

ØªØ¹Ù„ÙŠÙ…Ø§Øª:
1. Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙˆØ§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚
2. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ Ù‚Ù„ Ø£Ù†Ùƒ Ù„Ø§ ØªÙ…Ù„Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©
3. ÙƒÙ† Ù…ÙÙŠØ¯Ø§Ù‹ ÙˆÙ…Ù‡Ø°Ø¨Ø§Ù‹ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹
4. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
5. Ù„Ø§ ØªØ°ÙƒØ± Ø£Ù†Ùƒ ØªØ³ØªØ®Ø¯Ù… Ù…ØµØ§Ø¯Ø± Ù…Ø¹Ø±ÙÙŠØ©ØŒ ÙÙ‚Ø· Ø£Ø¬Ø¨ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:`
    
    const apiStart = performance.now()
    console.log('ğŸŒ [Gemini] Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¥Ù„Ù‰ API:', new Date().toISOString())
    
    const result = await model.generateContent(prompt)
    const response = await result.response
    
    const apiEnd = performance.now()
    const totalEnd = performance.now()
    
    console.log('âœ… [Gemini] Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ø±Ø¯ Ù…Ù† API:', new Date().toISOString())
    console.log('â±ï¸ [Gemini] Ø²Ù…Ù† API:', (apiEnd - apiStart).toFixed(2), 'ms')
    console.log('â±ï¸ [Gemini] Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ:', (totalEnd - performanceStart).toFixed(2), 'ms')
    console.log('ğŸ“ [Gemini] Ø·ÙˆÙ„ Ø§Ù„Ø±Ø¯:', response.text().length, 'Ø­Ø±Ù')
    
    return response.text()
  } catch (error) {
    const errorEnd = performance.now()
    console.error('âŒ [Gemini] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:', error)
    console.log('â±ï¸ [Gemini] Ø²Ù…Ù† Ø§Ù„Ø®Ø·Ø£:', (errorEnd - performanceStart).toFixed(2), 'ms')
    return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
  }
}

export async function POST(request: NextRequest) {
  const requestStart = performance.now()
  console.log('ğŸ¯ [Chat API] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨:', new Date().toISOString())
  
  try {
    const { message, botId, clientId } = await request.json()
    
    if (!message?.trim()) {
      return NextResponse.json(
        { error: "Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ø·Ù„ÙˆØ¨Ø©" },
        { status: 400 }
      )
    }
    
    if (!botId) {
      return NextResponse.json(
        { error: "Ù…Ø¹Ø±Ù Ø§Ù„Ø¨ÙˆØª Ù…Ø·Ù„ÙˆØ¨" },
        { status: 400 }
      )
    }
    
    // Get bot with related data
    const dbStart = performance.now()
    const bot = await prisma.bot.findUnique({
      where: { id: botId },
      include: {
        qas: {
          where: { isActive: true }
        },
        knowledgeSources: true
      }
    })
    const dbEnd = performance.now()
    console.log('ğŸ—„ï¸ [Database] Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙˆØª:', (dbEnd - dbStart).toFixed(2), 'ms')
    
    if (!bot) {
      return NextResponse.json(
        { error: "Ø§Ù„Ø¨ÙˆØª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯" },
        { status: 404 }
      )
    }
    
    if (!bot.isActive) {
      return NextResponse.json(
        { error: "Ø§Ù„Ø¨ÙˆØª ØºÙŠØ± Ù…ÙØ¹Ù„" },
        { status: 403 }
      )
    }
    
    // Get conversation history for context
    const historyStart = performance.now()
    const conversationHistory = await prisma.conversation.findMany({
      where: {
        botId: bot.id,
        clientId: clientId || 'anonymous'
      },
      orderBy: {
        createdAt: 'desc'
      },
      take: 5 // Ø¢Ø®Ø± 5 Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù„Ù„Ø³ÙŠØ§Ù‚
    })
    const historyEnd = performance.now()
    console.log('ğŸ“š [Database] Ø¬Ù„Ø¨ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:', (historyEnd - historyStart).toFixed(2), 'ms')
    console.log('ğŸ’¬ [Context] Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:', conversationHistory.length)
    
    let response = ""
    let responseType = "fallback"
    
    // Step 1: Check for Q&A match
    const qaStart = performance.now()
    const matchedQA = findSimilarQA(message, bot.qas)
    const qaEnd = performance.now()
    console.log('ğŸ” [Q&A Search] Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:', (qaEnd - qaStart).toFixed(2), 'ms')
    
    if (matchedQA) {
      console.log('âœ… [Q&A] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø·Ø§Ø¨Ù‚Ø©')
      response = matchedQA.answer
      responseType = "qa"
    } else if (bot.knowledgeSources.length > 0) {
      console.log('ğŸ¤– [RAG] Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini Ù…Ø¹ Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø¹Ø±ÙØ©')
      // Step 2: Use RAG with knowledge sources and conversation history
      response = await generateRAGResponse(message, bot, bot.knowledgeSources, conversationHistory.reverse())
      responseType = "rag"
    } else {
      console.log('âš ï¸ [Fallback] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ØµØ§Ø¯Ø± Ù…Ø¹Ø±ÙØ© Ù…ØªØ§Ø­Ø©')
      // Step 3: Fallback response
      response = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ Ø£Ù…Ù„Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ ÙØ±ÙŠÙ‚ Ø§Ù„Ø¯Ø¹Ù… Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹."
      responseType = "fallback"
    }
    
    // Save conversation using new session system
    const saveStart = performance.now()
    
    // Find or create conversation session
    let conversationSession = await prisma.conversationSession.findFirst({
      where: {
        botId: bot.id,
        clientId: clientId || 'anonymous'
      },
      orderBy: {
        updatedAt: 'desc'
      }
    })
    
    // Create new session if none exists or if last session is older than 1 hour
    const oneHourAgo = new Date(Date.now() - 60 * 60 * 1000)
    if (!conversationSession || conversationSession.updatedAt < oneHourAgo) {
      conversationSession = await prisma.conversationSession.create({
        data: {
          botId: bot.id,
          clientId: clientId || 'anonymous',
          title: message.slice(0, 50) + (message.length > 50 ? '...' : '')
        }
      })
    }
    
    // Save user message
    await prisma.message.create({
      data: {
        conversationSessionId: conversationSession.id,
        sender: 'user',
        content: message
      }
    })
    
    // Save bot response
    await prisma.message.create({
      data: {
        conversationSessionId: conversationSession.id,
        sender: 'bot',
        content: response,
        responseType
      }
    })
    
    // Update session timestamp
    await prisma.conversationSession.update({
      where: { id: conversationSession.id },
      data: { updatedAt: new Date() }
    })
    
    // Also save to old conversation table for backward compatibility
    await prisma.conversation.create({
      data: {
        botId: bot.id,
        clientId: clientId || 'anonymous',
        question: message,
        answer: response,
        responseType
      }
    })
    
    const saveEnd = performance.now()
    console.log('ğŸ’¾ [Database] Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:', (saveEnd - saveStart).toFixed(2), 'ms')
    
    const requestEnd = performance.now()
    console.log('ğŸ [Chat API] Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨:', (requestEnd - requestStart).toFixed(2), 'ms')
    console.log('ğŸ“Š [Summary] Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:', responseType)
    
    return NextResponse.json({
      response,
      responseType,
      botName: bot.name,
      performance: {
        total: (requestEnd - requestStart).toFixed(2),
        database: (dbEnd - dbStart + historyEnd - historyStart + saveEnd - saveStart).toFixed(2),
        processing: responseType === 'rag' ? 'measured_in_generateRAGResponse' : (qaEnd - qaStart).toFixed(2)
      }
    })
    
  } catch (error) {
    console.error('Error in chat API:', error)
    return NextResponse.json(
      { error: "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø®Ø§Ø¯Ù…" },
      { status: 500 }
    )
  }
}